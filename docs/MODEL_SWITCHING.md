# üîÑ Troca de Modelos AI em Tempo Real

Este documento explica como usar a funcionalidade de troca de modelos AI durante conversas.

## üìã √çndice

- [Modelos Dispon√≠veis](#modelos-dispon√≠veis)
- [Como Funciona](#como-funciona)
- [Uso no Backend](#uso-no-backend)
- [Uso no Frontend](#uso-no-frontend)
- [Exemplos Pr√°ticos](#exemplos-pr√°ticos)

## ü§ñ Modelos Dispon√≠veis

| Provider | Modelo | ID | Especialidade | Context Window |
|----------|--------|----|--------------  |----------------|
| **Google** | Gemini 2.5 Pro | `google/gemini-2.5-pro` | Geral, vis√£o | 2M tokens |
| **XAI** | Grok 2 | `xai/grok-2-1212` | Conversas naturais | 131K tokens |
| **ZAI** | GLM-4.6 | `zai/glm-4.6` | C√≥digo, agentes | 200K tokens |
| **OpenAI** | GPT-4o | `openai/gpt-4o` | Vers√°til | 128K tokens |
| **Anthropic** | Claude 3.5 Sonnet | `anthropic/claude-3-5-sonnet-20241022` | An√°lise profunda | 200K tokens |

## ‚öôÔ∏è Configura√ß√£o

### Vari√°veis de Ambiente

Adicione as credenciais no `.env.local`:

```bash
# Provider principal (padr√£o para toda aplica√ß√£o)
AI_SDK_PROVIDER=google

# Google (Gemini)
GEMINI_API_KEY=your_gemini_key

# XAI (Grok)
XAI_API_KEY=your_xai_key
XAI_MODEL=grok-2-1212

# ZAI (GLM)
ZAI_API_KEY=your_zai_key
ZAI_MODEL=glm-4.6

# OpenAI
OPENAI_API_KEY=your_openai_key

# Anthropic
ANTHROPIC_API_KEY=your_anthropic_key
```

## üîß Como Funciona

### 1. Arquitetura

```
Cliente ‚Üí API Endpoint ‚Üí ChatService ‚Üí AIProvider ‚Üí Modelo AI
                ‚Üì
        { provider, model }
```

### 2. Fluxo de Troca

1. **Usu√°rio seleciona modelo** no UI (ModelSelector)
2. **Cliente envia par√¢metros** `provider` e `model` na requisi√ß√£o
3. **ChatService** detecta par√¢metros e passa para AIProvider
4. **AIProvider** carrega o modelo solicitado
5. **Hist√≥rico √© mantido** entre trocas de modelo

## üíª Uso no Backend

### ChatService (Server-Side)

```typescript
import { ChatService } from '@/server/ai/chat-service';

const chatService = ChatService.getInstance();

// Usar modelo padr√£o
const result = await chatService.chat('Ol√°!', {
  history: chatHistory,
  userId: 'user-123'
});

// Trocar para Grok
const result = await chatService.chat('Continue...', {
  history: chatHistory,
  userId: 'user-123',
  provider: 'xai',
  model: 'grok-2-1212'
});

// Trocar para GLM-4.6 (especialista em c√≥digo)
const result = await chatService.chat('Escreva c√≥digo...', {
  history: chatHistory,
  userId: 'user-123',
  provider: 'zai',
  model: 'glm-4.6'
});
```

### AIProvider (Direto)

```typescript
import { getAISDKModel } from '@/server/aiProvider';
import { generateText } from 'ai';

// Usar modelo espec√≠fico
const model = await getAISDKModel({
  provider: 'xai',
  model: 'grok-2-1212'
});

const response = await generateText({
  model,
  prompt: 'Ol√°!',
  maxTokens: 100
});
```

## üé® Uso no Frontend

### Componente ModelSelector

```tsx
import { ModelSelector } from '@/components/ModelSelector';
import { useState } from 'react';

function ChatPanel() {
  const [selectedModel, setSelectedModel] = useState('google/gemini-2.5-pro');

  return (
    <div>
      <ModelSelector
        value={selectedModel}
        onChange={setSelectedModel}
        context="chat" // ou 'code', 'analysis', etc.
      />
      {/* Chat UI */}
    </div>
  );
}
```

### API Endpoint

```typescript
// POST /api/chat/for-item
const response = await fetch('/api/chat/for-item', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    title: 'Minha Tarefa',
    type: 'Tarefa',
    message: 'Ol√°!',
    history: chatHistory,
    // Trocar modelo em tempo real
    provider: 'xai',
    model: 'grok-2-1212'
  })
});
```

## üìù Exemplos Pr√°ticos

### Exemplo 1: Conversa Multi-Modelo

```typescript
const conversation = [];

// 1. Come√ßar com Gemini
const response1 = await chatService.chat('O que √© IA?', {
  provider: 'google',
  model: 'gemini-2.5-pro'
});
conversation.push(
  { role: 'user', content: 'O que √© IA?' },
  { role: 'assistant', content: response1.text }
);

// 2. Trocar para Grok para conversa
const response2 = await chatService.chat('Me d√™ exemplos pr√°ticos', {
  history: conversation,
  provider: 'xai',
  model: 'grok-2-1212'
});
conversation.push(
  { role: 'user', content: 'Me d√™ exemplos pr√°ticos' },
  { role: 'assistant', content: response2.text }
);

// 3. Trocar para GLM-4.6 para c√≥digo
const response3 = await chatService.chat('Escreva c√≥digo Python', {
  history: conversation,
  provider: 'zai',
  model: 'glm-4.6'
});
```

### Exemplo 2: Especializa√ß√£o por Tarefa

```typescript
function selectModelForTask(taskType: string) {
  const modelMap = {
    'code': { provider: 'zai', model: 'glm-4.6' },
    'conversation': { provider: 'xai', model: 'grok-2-1212' },
    'analysis': { provider: 'anthropic', model: 'claude-3-5-sonnet-20241022' },
    'general': { provider: 'google', model: 'gemini-2.5-pro' }
  };

  return modelMap[taskType] || modelMap['general'];
}

// Uso
const modelConfig = selectModelForTask('code');
const response = await chatService.chat('Write a function...', {
  ...modelConfig
});
```

## üéØ Casos de Uso

### Quando usar cada modelo:

**Gemini 2.5 Pro** (`google/gemini-2.5-pro`)
- ‚úÖ Tarefas gerais
- ‚úÖ An√°lise de documentos longos
- ‚úÖ Processamento de imagens
- ‚úÖ Grande contexto (2M tokens)

**Grok 2** (`xai/grok-2-1212`)
- ‚úÖ Conversas naturais e criativas
- ‚úÖ Resposta r√°pida
- ‚úÖ Informa√ß√µes atualizadas (integra√ß√£o X)
- ‚úÖ Custo eficiente

**GLM-4.6** (`zai/glm-4.6`)
- ‚úÖ Gera√ß√£o de c√≥digo
- ‚úÖ Tarefas de agentes
- ‚úÖ Debugging e an√°lise t√©cnica
- ‚úÖ 30% mais eficiente em tokens

**GPT-4o** (`openai/gpt-4o`)
- ‚úÖ Versatilidade
- ‚úÖ Function calling
- ‚úÖ An√°lise de imagens
- ‚úÖ Racioc√≠nio complexo

**Claude 3.5 Sonnet** (`anthropic/claude-3-5-sonnet-20241022`)
- ‚úÖ An√°lise profunda
- ‚úÖ Escrita criativa
- ‚úÖ Racioc√≠nio √©tico
- ‚úÖ Context window grande

## üîç Debugging

### Verificar modelo atual

```typescript
const { model, metadata } = await AIProvider.getInstance()
  .getModelForContext('chat', {
    provider: 'xai',
    model: 'grok-2-1212'
  });

console.log('Provider:', metadata?.provider);
console.log('Model:', metadata?.modelId);
```

### Logs

O sistema loga automaticamente as trocas de modelo:

```
[ChatService] Usando modelo customizado { provider: 'xai', model: 'grok-2-1212' }
```

## üö® Limita√ß√µes

1. **Cr√©ditos**: Cada provider requer cr√©ditos pr√≥prios
2. **Rate Limits**: Respeite os limites de cada API
3. **Context Window**: N√£o exceda o limite de cada modelo
4. **Capabilities**: Nem todos modelos suportam vision/function-calling

## üìö Refer√™ncias

- [Vercel AI SDK Docs](https://ai-sdk.dev)
- [Google Gemini API](https://ai.google.dev)
- [XAI Grok Docs](https://docs.x.ai)
- [Zhipu AI Docs](https://docs.z.ai)
- [OpenAI API](https://platform.openai.com)
- [Anthropic Claude](https://docs.anthropic.com)

---

**üéâ Feature implementada!** Agora voc√™ pode trocar modelos AI em tempo real durante conversas, mantendo todo o contexto e hist√≥rico.
